# Fine-Grained Food Classification using an Ensemble of Deep Learning Models

## Project Overview

This repository contains the complete code and documentation for a project developed for the **iFood 2019 FGVC6 Kaggle competition**. The primary objective was to tackle a challenging fine-grained visual categorization (FGVC) task: classifying images of food into one of **251 distinct categories**.

This project was part of the 6th Fine-Grained Visual Categorization workshop at the **CVPR 2019 conference**.

### Key Challenges

The competition presented two main difficulties that this project aimed to solve:

1.  **Fine-Grained Classes:** The dataset includes highly specific and visually similar categories (e.g., 15 different types of cake, 10 different types of pasta), which requires a model capable of discerning subtle visual features.
2.  **Noisy Data:** The training images were sourced from the web and contained significant noise, such as images of raw ingredients, packaged food items, or multiple food items in a single frame.

### Evaluation Metric

Submissions were evaluated based on the **mean top-3 accuracy**. A prediction was considered correct if the true ground-truth label was present within the model's top three most confident predictions.

---

## Solution Strategy

To address these challenges, this project implements a robust, multi-stage pipeline utilizing a **model ensembling** strategy. This approach combines the predictive power of multiple diverse models to produce a more accurate and resilient final result.

The core components of this strategy are:

1.  **Transfer Learning with Diverse Architectures:** We leverage two state-of-the-art, pre-trained convolutional neural networks (CNNs):
    * **DenseNet201**
    * **InceptionResNetV2**

    By using two different, powerful architectures, we encourage the models to learn slightly different representations of the data, making them ideal candidates for ensembling.

2.  **Data Augmentation:** To make the models more robust to the noisy training data and prevent overfitting, the training dataset was artificially expanded by applying random transformations to the images (e.g., rotation, shifting, zooming, and horizontal flipping).

3.  **Model Ensembling:** The final predictions are generated by averaging the class probabilities from the two independently trained models. This method is particularly effective for top-3 accuracy, as it smooths out individual model errors and increases the likelihood that the correct label appears in the top predictions.

---



* **`Kaggle_iFood_2019_Showcase_Project.ipynb`**: The main, consolidated Jupyter Notebook that combines all steps of the project. It includes data preparation, training for both models, and the final ensembling and prediction logic. This is the primary file for review.
* **`original_notebooks/`**: This directory contains the three original, separate notebooks used during the development process.
* **`README.md`**: This file, providing an overview of the project.

---

## How to Run

This project was developed using **Google Colab** with a GPU accelerator.

### Prerequisites

* Python 3
* TensorFlow / Keras
* Pandas, NumPy, Matplotlib
* A Google account with access to Google Drive (for saving and loading model weights).

### Instructions

1.  **Clone the Repository:**
    ```bash
    git clone [URL_to_your_repository]
    cd [repository_name]
    ```

2.  **Open in Google Colab:**
    * Upload the `Kaggle_iFood_2019_Showcase_Project.ipynb` notebook to your Google Colab environment.
    * Ensure the runtime is set to use a **GPU accelerator** (`Runtime` -> `Change runtime type` -> `GPU`).

3.  **Run the Notebook:**
    * The notebook is self-contained. The first cells will mount your Google Drive and download the necessary competition data from an AWS S3 bucket.
    * **Training:** The training cells for both DenseNet201 and InceptionResNetV2 are included. **Note:** Training these models is computationally intensive and time-consuming. The notebook is set to run for only a few epochs for demonstration. The original training was performed for 50 and 5 epochs, respectively.
    * **Ensembling:** If you have already trained the models and saved the weights (`.hdf5` files) and architectures (`.json` files) to your Google Drive, you can skip the training cells and run the "Model Ensembling and Final Prediction" section directly.
    * **Output:** The final cell will generate a `submission.csv` file formatted for the Kaggle competition.

---

## Conclusion

This project demonstrates a complete and effective workflow for tackling a complex, fine-grained image classification problem. The strategic use of transfer learning, data augmentation, and model ensembling represents a robust and high-performing approach, well-suited for real-world computer vision challenges.

