# Fine-Grained Food Classification using an Ensemble of Deep Learning Models

## Project Overview

This repository contains the complete code and documentation for a project developed for the **iFood 2019 FGVC6 Kaggle competition**. The primary objective was to tackle a challenging fine-grained visual categorization (FGVC) task: classifying images of food into one of **251 distinct categories**.

This project was part of the 6th Fine-Grained Visual Categorization workshop at the **CVPR 2019 conference**.

### Results & Achievements

This solution achieved a final placement of **18th out of 40 teams** on the private leaderboard.

* **Final Score:** 0.12812 (Mean Top-3 Accuracy Error)
* **Ranking:** 18 / 40

This result demonstrates the effectiveness of the ensembling strategy in handling fine-grained classification with noisy data.

Link to the competition on Kaggle: https://www.kaggle.com/competitions/ifood-2019-fgvc6

### Core Challenges

The competition presented two primary difficulties that this project aimed to solve:

1.  **Fine-Grained & Visually Similar Classes:** The dataset includes highly specific categories with low inter-class variance (e.g., 15 types of cake, 10 different types of pasta), requiring a model capable of discerning very subtle visual features.
2.  **Noisy Training Data:** The training images were scraped from the web and contained significant noise, such as images of raw ingredients, packaged food items, or multiple food items in a single frame.

### Solution Strategy

To address these challenges, this project implements a robust, multi-stage pipeline utilizing a **model ensembling** strategy.

1.  **Transfer Learning with Diverse Architectures:** We leverage two state-of-the-art, pre-trained convolutional neural networks (CNNs):
    * **DenseNet201**
    * **InceptionResNetV2**

2.  **Data Augmentation:** To make the models more robust and prevent overfitting, the training dataset was artificially expanded by applying random transformations (e.g., rotation, shifting, zooming, and horizontal flipping).

3.  **Model Ensembling:** The final predictions are generated by averaging the class probabilities from the two independently trained models. This method is particularly effective for top-3 accuracy, as it smooths out individual model errors and increases the likelihood that the correct label appears in the top predictions.

---

## Repository Structure

* **`Kaggle_iFood_2019_Showcase_Project.ipynb`**: The main, consolidated Jupyter Notebook that combines all steps of the project. It includes data preparation, training for both models, and the final ensembling and prediction logic. This is the primary file for review.
* **`original_notebooks/`**: This directory contains the three original, separate notebooks used during the development process.
* **`README.md`**: This file, providing an overview of the project.

---

## How to Run

This project was developed using **Google Colab** with a GPU accelerator.

### Prerequisites

* Python 3
* TensorFlow / Keras
* Pandas, NumPy, Matplotlib
* A Google account with access to Google Drive (for saving and loading model weights).

### Instructions

1.  **Clone the Repository:**
    ```bash
    git clone [URL_to_your_repository]
    cd [repository_name]
    ```

2.  **Open in Google Colab:**
    * Upload the `Kaggle_iFood_2019_Showcase_Project.ipynb` notebook to your Google Colab environment.
    * Ensure the runtime is set to use a **GPU accelerator** (`Runtime` -> `Change runtime type` -> `GPU`).

3.  **Run the Notebook:**
    * The notebook is self-contained. The first cells will mount your Google Drive and download the necessary competition data from an AWS S3 bucket.
    * **Training:** The training cells for both DenseNet201 and InceptionResNetV2 are included. **Note:** Training these models is computationally intensive and time-consuming. The notebook is set to run for only a few epochs for demonstration. The original training was performed for 50 and 5 epochs, respectively.
    * **Ensembling:** If you have already trained the models and saved the weights (`.hdf5` files) and architectures (`.json` files) to your Google Drive, you can skip the training cells and run the "Model Ensembling and Final Submission" section directly.
    * **Output:** The final cell will generate a `submission.csv` file formatted for the Kaggle competition.

---

## Final Conclusion

This project demonstrates a complete and effective workflow for tackling a complex, fine-grained image classification problem. The strategic use of transfer learning, data augmentation, and model ensembling resulted in a high-performing solution, achieving a top-20 finish on the official Kaggle private leaderboard.
