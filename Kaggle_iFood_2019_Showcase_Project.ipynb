{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Kaggle_iFood_2019_Showcase_Project.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# iFood 2019 Kaggle Competition: An Ensemble-Based Approach\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "This notebook presents a comprehensive, end-to-end solution for the **iFood 2019 FGVC6 Kaggle competition**. The objective was to address the challenging task of fine-grained image classification: identifying the specific food category from one of **251 classes** given a single image.\n",
    "\n",
    "This project was developed as part of the 6th Fine-Grained Visual Categorization workshop (FGVC6) at CVPR 2019.\n",
    "\n",
    "### Core Challenges\n",
    "The competition presented two primary difficulties, as outlined on the Kaggle page:\n",
    "\n",
    "1.  **Fine-Grained & Visually Similar Classes:** The dataset includes highly specific categories with low inter-class variance. For example, it contains 15 different types of cake and 10 different types of pasta, requiring a model capable of discerning very subtle visual features.\n",
    "2.  **Noisy Training Data:** The training images were scraped from the web, leading to significant cross-domain noise. This included images of raw ingredients, packaged food items, or multiple food items in a single frame, all of which could dilute the features learned by the model.\n",
    "\n",
    "### Evaluation Metric\n",
    "Submissions were evaluated based on the **mean top-3 accuracy**. This means a prediction for an image was considered correct if the true ground-truth label was present within the model's top three most confident predictions.\n",
    "\n",
    "### Solution Strategy\n",
    "\n",
    "To tackle these challenges, this project implements a robust, multi-stage pipeline utilizing advanced deep learning techniques:\n",
    "\n",
    "1.  **Transfer Learning with Diverse Architectures:** We leverage two state-of-the-art, pre-trained convolutional neural networks (CNNs): **DenseNet201** and **InceptionResNetV2**. By using two different architectures, we encourage the models to learn slightly different features, making them ideal for ensembling.\n",
    "2.  **Data Augmentation:** To make the models more robust to the noisy training data and prevent overfitting, we artificially expand the dataset by applying random transformations (e.g., rotation, shifting, zooming). This teaches the models to be invariant to these visual changes.\n",
    "3.  **Model Ensembling:** The final predictions are generated by averaging the outputs of the two independently trained models. This method is particularly effective for top-3 accuracy, as it smooths out individual model errors and increases the probability that the correct label appears in the top predictions.\n",
    "\n",
    "### Dataset and Files\n",
    "The competition provided the following files:\n",
    "- **`{train/val/test}.zip`**: The image datasets for training, validation, and testing.\n",
    "- **`class_list.txt`**: A lookup table mapping the numeric class labels to their human-readable names (e.g., '10' -> 'knish').\n",
    "- **`{train/val}_labels.csv`**: CSV files containing the image names and their corresponding ground-truth labels for the training and validation sets.\n",
    "- **`test_info.csv`**: A CSV file containing the names of the test images."
   ],
   "metadata": {
    "id": "intro_markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Part 1: Setup and Data Preparation"
   ],
   "metadata": {
    "id": "part_1_header"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.1. Mount Google Drive and Import Libraries\n",
    "We begin by mounting Google Drive to access saved model files and importing all the necessary Python libraries for data manipulation, machine learning, and visualization."
   ],
   "metadata": {
    "id": "part_1_1_markdown"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Connect to Google Drive to save/load models\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "# General and data manipulation libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# TensorFlow and Keras for deep learning\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.applications import DenseNet201, InceptionResNetV2\n",
    "\n",
    "# Plotting and visualization\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ],
   "metadata": {
    "id": "imports_code"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.2. Download and Extract Competition Data\n",
    "The dataset is downloaded from its source on AWS S3. The tar archives are then extracted, and the original archives are removed to save disk space on the Colab environment."
   ],
   "metadata": {
    "id": "part_1_2_markdown"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Download and extract dataset from the iFood 2019 competition\n",
    "!wget https://food-x.s3.amazonaws.com/annot.tar\n",
    "!tar -xf annot.tar\n",
    "!rm -r annot.tar\n",
    "\n",
    "!wget https://food-x.s3.amazonaws.com/train.tar\n",
    "!tar -xf train.tar\n",
    "!rm -r train.tar\n",
    "\n",
    "!wget https://food-x.s3.amazonaws.com/val.tar\n",
    "!tar -xf val.tar\n",
    "!rm -r val.tar\n",
    "\n",
    "!wget https://food-x.s3.amazonaws.com/test.tar  \n",
    "!tar -xf test.tar\n",
    "!rm -r test.tar"
   ],
   "metadata": {
    "id": "download_data_code"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.3. Load Labels and Define Directories\n",
    "We load the image file names and their corresponding labels from the provided CSV files into pandas DataFrames. These DataFrames will be used by Keras data generators to efficiently feed images to the models. The dataset consists of 118,475 training images, 11,994 validation images, and 28,377 test images."
   ],
   "metadata": {
    "id": "part_1_3_markdown"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Define base directories for the data\n",
    "base_dir = '.'\n",
    "train_dir = os.path.join(base_dir, 'train_set')\n",
    "validation_dir = os.path.join(base_dir, 'val_set')\n",
    "test_dir = os.path.join(base_dir, 'test_set')\n",
    "\n",
    "# Load labels from CSV files\n",
    "train_labels = pd.read_csv(os.path.join(base_dir, 'train_info.csv'), header=None)\n",
    "train_labels.columns = [\"img_name\", \"label\"]\n",
    "\n",
    "val_labels = pd.read_csv(os.path.join(base_dir, 'val_info.csv'), header=None)\n",
    "val_labels.columns = [\"img_name\", \"label\"]\n",
    "\n",
    "test_labels = pd.read_csv(os.path.join(base_dir, 'test_info.csv'), header=None)\n",
    "test_labels.columns = [\"img_name\"]\n",
    "\n",
    "# Convert label columns to string type, as expected by flow_from_dataframe\n",
    "train_labels['label'] = train_labels['label'].astype(str)\n",
    "val_labels['label'] = val_labels['label'].astype(str)\n",
    "\n",
    "print(f\"Training samples: {len(train_labels)}\")\n",
    "print(f\"Validation samples: {len(val_labels)}\")\n",
    "print(f\"Test samples: {len(test_labels)}\")"
   ],
   "metadata": {
    "id": "load_labels_code"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Part 2: Training Model A (DenseNet201)"
   ],
   "metadata": {
    "id": "part_2_header"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.1. Hyperparameters and Data Generators for DenseNet201\n",
    "\n",
    "We define the specific parameters for the DenseNet201 model. A key parameter here is `image_size`, which is set to **224x224**, the standard input size for this architecture.\n",
    "\n",
    "We then create the `ImageDataGenerator`. For the training set, we apply a range of augmentations to make the model more robust. The validation data is only rescaled, as it should remain unaltered to provide a true measure of performance."
   ],
   "metadata": {
    "id": "part_2_1_markdown"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# --- Model-specific Hyperparameters ---\n",
    "NUM_CLASSES = 251\n",
    "BATCH_SIZE_DENSENET = 128\n",
    "LEARNING_RATE_DENSENET = 0.001\n",
    "NUM_EPOCHS_DENSENET = 50 # The original model was trained for 50 epochs\n",
    "IMAGE_SIZE_DENSENET = 224\n",
    "\n",
    "# --- Data Augmentation and Generators ---\n",
    "\n",
    "# Create an ImageDataGenerator for the training set with augmentation\n",
    "train_datagen_densenet = ImageDataGenerator(\n",
    "    rescale=1./255.,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "# The validation data should not be augmented, only rescaled\n",
    "validation_datagen_densenet = ImageDataGenerator(rescale=1.0/255.)\n",
    "\n",
    "# Flow images in batches using the generators\n",
    "train_generator_densenet = train_datagen_densenet.flow_from_dataframe(\n",
    "    dataframe=train_labels,\n",
    "    directory=train_dir,\n",
    "    x_col='img_name',\n",
    "    y_col='label',\n",
    "    class_mode='categorical',\n",
    "    batch_size=BATCH_SIZE_DENSENET,\n",
    "    target_size=(IMAGE_SIZE_DENSENET, IMAGE_SIZE_DENSENET)\n",
    ")\n",
    "\n",
    "validation_generator_densenet = validation_datagen_densenet.flow_from_dataframe(\n",
    "    dataframe=val_labels,\n",
    "    directory=validation_dir,\n",
    "    x_col='img_name',\n",
    "    y_col='label',\n",
    "    class_mode='categorical',\n",
    "    batch_size=BATCH_SIZE_DENSENET,\n",
    "    target_size=(IMAGE_SIZE_DENSENET, IMAGE_SIZE_DENSENET)\n",
    ")"
   ],
   "metadata": {
    "id": "densenet_hyperparams_code"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2. Build DenseNet201 Model with Transfer Learning\n",
    "\n",
    "We load the `DenseNet201` model, pre-trained on ImageNet (`weights='imagenet'`). The weights of the convolutional base are frozen (`layer.trainable = False`) to leverage its powerful, learned features without altering them during initial training.\n",
    "\n",
    "We then add a new classification \"head\" on top. This consists of a `GlobalAveragePooling2D` layer to reduce the feature dimensions, followed by a final `Dense` layer with 251 outputs and a `softmax` activation function. This new head will be trained from scratch on our food dataset."
   ],
   "metadata": {
    "id": "part_2_2_markdown"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Load the DenseNet201 model with weights pre-trained on ImageNet\n",
    "# We exclude the final classification layer (include_top=False)\n",
    "pre_trained_model_densenet = DenseNet201(\n",
    "    input_shape=(IMAGE_SIZE_DENSENET, IMAGE_SIZE_DENSENET, 3),\n",
    "    include_top=False,\n",
    "    weights='imagenet'\n",
    ")\n",
    "\n",
    "# Freeze the convolutional base to prevent its weights from being updated\n",
    "for layer in pre_trained_model_densenet.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add a new classification head\n",
    "x = layers.GlobalAveragePooling2D()(pre_trained_model_densenet.output)\n",
    "x = layers.Dense(NUM_CLASSES, activation='softmax')(x)\n",
    "\n",
    "# Create the final model\n",
    "model_densenet = Model(pre_trained_model_densenet.input, x)\n",
    "\n",
    "model_densenet.summary()"
   ],
   "metadata": {
    "id": "densenet_build_code"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.3. Compile and Train the DenseNet201 Model\n",
    "\n",
    "The model is compiled with the SGD optimizer and `categorical_crossentropy` loss, suitable for multi-class classification. We use a `ModelCheckpoint` callback to automatically save the model with the best validation accuracy (`val_acc`) during training.\n",
    "\n",
    "**Note:** The following cell will initiate the training process. For demonstration purposes, it is set to run for only a few epochs. The original model was trained for 50 epochs."
   ],
   "metadata": {
    "id": "part_2_3_markdown"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Compile the model\n",
    "model_densenet.compile(\n",
    "    optimizer=SGD(lr=LEARNING_RATE_DENSENET, momentum=0.9, decay=1e-6),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['acc']\n",
    ")\n",
    "\n",
    "# Define the checkpoint to save the best model\n",
    "checkpoint_densenet = ModelCheckpoint(\n",
    "    filepath=\"/content/gdrive/My Drive/iFood2019_DenseNet201_best.hdf5\",\n",
    "    monitor='val_acc',\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    mode='max'\n",
    ")\n",
    "\n",
    "# --- TRAINING --- \n",
    "# Note: Training is computationally intensive. We will run it for just 2 epochs for this demo.\n",
    "history_densenet = model_densenet.fit(\n",
    "    train_generator_densenet,\n",
    "    validation_data=validation_generator_densenet,\n",
    "    epochs=2, # Set to 2 for demo; original was 50\n",
    "    verbose=1,\n",
    "    callbacks=[checkpoint_densenet]\n",
    ")"
   ],
   "metadata": {
    "id": "densenet_compile_train_code"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.4. Save Model Architecture\n",
    "After training, we save the model's architecture as a JSON file. This allows us to recreate the model structure later without retraining, loading only the saved weights."
   ],
   "metadata": {
    "id": "part_2_4_markdown"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Serialize model architecture to JSON\n",
    "model_json_densenet = model_densenet.to_json()\n",
    "with open(\"/content/gdrive/My Drive/iFood2019_DenseNet201_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json_densenet)\n",
    "    \n",
    "print(\"DenseNet201 model architecture saved to Google Drive.\")"
   ],
   "metadata": {
    "id": "densenet_save_code"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Part 3: Training Model B (InceptionResNetV2)"
   ],
   "metadata": {
    "id": "part_3_header"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.1. Hyperparameters and Data Generators for InceptionResNetV2\n",
    "\n",
    "Now we set up the second model. The key difference is the `IMAGE_SIZE_INCEPTION`, which is **299x299** as required by this architecture. We create a new set of data generators tailored to this image size."
   ],
   "metadata": {
    "id": "part_3_1_markdown"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# --- Model-specific Hyperparameters ---\n",
    "BATCH_SIZE_INCEPTION = 64 # Smaller batch size due to larger model size\n",
    "LEARNING_RATE_INCEPTION = 0.001\n",
    "NUM_EPOCHS_INCEPTION = 5 # The original model was trained for 5 epochs\n",
    "IMAGE_SIZE_INCEPTION = 299 # Inception architecture requires 299x299 images\n",
    "\n",
    "# --- Data Augmentation and Generators ---\n",
    "\n",
    "# The training data generator uses the same augmentation strategy\n",
    "train_datagen_inception = ImageDataGenerator(\n",
    "    rescale=1./255.,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "# Validation data is only rescaled\n",
    "validation_datagen_inception = ImageDataGenerator(rescale=1.0/255.)\n",
    "\n",
    "# Flow images in batches using the new generators\n",
    "train_generator_inception = train_datagen_inception.flow_from_dataframe(\n",
    "    dataframe=train_labels,\n",
    "    directory=train_dir,\n",
    "    x_col='img_name',\n",
    "    y_col='label',\n",
    "    class_mode='categorical',\n",
    "    batch_size=BATCH_SIZE_INCEPTION,\n",
    "    target_size=(IMAGE_SIZE_INCEPTION, IMAGE_SIZE_INCEPTION)\n",
    ")\n",
    "\n",
    "validation_generator_inception = validation_datagen_inception.flow_from_dataframe(\n",
    "    dataframe=val_labels,\n",
    "    directory=validation_dir,\n",
    "    x_col='img_name',\n",
    "    y_col='label',\n",
    "    class_mode='categorical',\n",
    "    batch_size=BATCH_SIZE_INCEPTION,\n",
    "    target_size=(IMAGE_SIZE_INCEPTION, IMAGE_SIZE_INCEPTION)\n",
    ")"
   ],
   "metadata": {
    "id": "inception_hyperparams_code"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.2. Build InceptionResNetV2 Model with Transfer Learning\n",
    "\n",
    "The process mirrors the one for DenseNet201. We load the pre-trained `InceptionResNetV2` base, freeze its layers, and attach a new classification head suitable for our 251 food categories."
   ],
   "metadata": {
    "id": "part_3_2_markdown"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Load the InceptionResNetV2 model with weights pre-trained on ImageNet\n",
    "pre_trained_model_inception = InceptionResNetV2(\n",
    "    input_shape=(IMAGE_SIZE_INCEPTION, IMAGE_SIZE_INCEPTION, 3),\n",
    "    include_top=False,\n",
    "    weights='imagenet'\n",
    ")\n",
    "\n",
    "# Freeze the convolutional base\n",
    "for layer in pre_trained_model_inception.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add the new classification head\n",
    "y = layers.GlobalAveragePooling2D()(pre_trained_model_inception.output)\n",
    "y = layers.Dense(NUM_CLASSES, activation='softmax')(y)\n",
    "\n",
    "# Create the final model\n",
    "model_inception = Model(pre_trained_model_inception.input, y)\n",
    "\n",
    "model_inception.summary()"
   ],
   "metadata": {
    "id": "inception_build_code"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.3. Compile and Train the InceptionResNetV2 Model\n",
    "\n",
    "We compile and train the second model. The `ModelCheckpoint` will save the best-performing version of this model to a separate file on Google Drive.\n",
    "\n",
    "**Note:** Training is again limited to 2 epochs for this demonstration. The original model was trained for 5 epochs."
   ],
   "metadata": {
    "id": "part_3_3_markdown"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Compile the model\n",
    "model_inception.compile(\n",
    "    optimizer=SGD(lr=LEARNING_RATE_INCEPTION, momentum=0.9, decay=1e-6),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['acc']\n",
    ")\n",
    "\n",
    "# Define the checkpoint to save the best model\n",
    "checkpoint_inception = ModelCheckpoint(\n",
    "    filepath=\"/content/gdrive/My Drive/iFood2019_InceptionResNetV2_best.hdf5\",\n",
    "    monitor='val_acc',\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    mode='max'\n",
    ")\n",
    "\n",
    "# --- TRAINING --- \n",
    "# Note: Set to 2 epochs for demo purposes.\n",
    "history_inception = model_inception.fit(\n",
    "    train_generator_inception,\n",
    "    validation_data=validation_generator_inception,\n",
    "    epochs=2, # Set to 2 for demo; original was 5\n",
    "    verbose=1,\n",
    "    callbacks=[checkpoint_inception]\n",
    ")"
   ],
   "metadata": {
    "id": "inception_compile_train_code"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4. Save Model Architecture\n",
    "We save the architecture of the trained InceptionResNetV2 model."
   ],
   "metadata": {
    "id": "part_3_4_markdown"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Serialize model architecture to JSON\n",
    "model_json_inception = model_inception.to_json()\n",
    "with open(\"/content/gdrive/My Drive/iFood2019_InceptionResNetV2_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json_inception)\n",
    "    \n",
    "print(\"InceptionResNetV2 model architecture saved to Google Drive.\")"
   ],
   "metadata": {
    "id": "inception_save_code"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Part 4: Model Ensembling and Final Submission\n",
    "\n",
    "In this final part, we combine the predictive power of our two trained models. The core idea of ensembling is that by averaging the predictions of multiple diverse models, we can reduce variance and produce a more accurate and robust final result."
   ],
   "metadata": {
    "id": "part_4_header"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.1. Load Trained Models\n",
    "\n",
    "We first recreate the model architectures from the saved JSON files and then load the best-performing weights that were saved by the `ModelCheckpoint` during training."
   ],
   "metadata": {
    "id": "part_4_1_markdown"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from tensorflow.keras.models import model_from_json\n",
    "\n",
    "# Define paths to the saved models on Google Drive\n",
    "gdrive_path = \"/content/gdrive/My Drive/\"\n",
    "densenet_model_path = os.path.join(gdrive_path, \"iFood2019_DenseNet201_model.json\")\n",
    "densenet_weights_path = os.path.join(gdrive_path, \"iFood2019_DenseNet201_best.hdf5\")\n",
    "\n",
    "inception_model_path = os.path.join(gdrive_path, \"iFood2019_InceptionResNetV2_model.json\")\n",
    "inception_weights_path = os.path.join(gdrive_path, \"iFood2019_InceptionResNetV2_best.hdf5\")\n",
    "\n",
    "# --- Load DenseNet201 ---\n",
    "print(\"Loading DenseNet201 from disk...\")\n",
    "with open(densenet_model_path, 'r') as json_file:\n",
    "    loaded_model_json = json_file.read()\n",
    "densenet_model = model_from_json(loaded_model_json)\n",
    "densenet_model.load_weights(densenet_weights_path)\n",
    "\n",
    "# --- Load InceptionResNetV2 ---\n",
    "print(\"Loading InceptionResNetV2 from disk...\")\n",
    "with open(inception_model_path, 'r') as json_file:\n",
    "    loaded_model_json = json_file.read()\n",
    "inception_model = model_from_json(loaded_model_json)\n",
    "inception_model.load_weights(inception_weights_path)\n",
    "\n",
    "print(\"\\nModels loaded successfully.\")"
   ],
   "metadata": {
    "id": "ensemble_load_code"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.2. Create Test Data Generators\n",
    "\n",
    "Because our two models require different input image sizes, we must create two separate data generators for the test set. Importantly, we set `shuffle=False` to ensure that the predictions maintain the original order of the test images, which is critical for a correct submission."
   ],
   "metadata": {
    "id": "part_4_2_markdown"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Create a data generator for the test set (no augmentation, only rescaling)\n",
    "test_datagen = ImageDataGenerator(rescale=1.0/255.)\n",
    "\n",
    "# Generator for DenseNet201 (224x224)\n",
    "test_generator_densenet = test_datagen.flow_from_dataframe(\n",
    "    dataframe=test_labels,\n",
    "    directory=test_dir,\n",
    "    x_col='img_name',\n",
    "    class_mode=None, # No labels for the test set\n",
    "    batch_size=BATCH_SIZE_DENSENET,\n",
    "    shuffle=False, # Crucial for submission\n",
    "    target_size=(IMAGE_SIZE_DENSENET, IMAGE_SIZE_DENSENET)\n",
    ")\n",
    "\n",
    "# Generator for InceptionResNetV2 (299x299)\n",
    "test_generator_inception = test_datagen.flow_from_dataframe(\n",
    "    dataframe=test_labels,\n",
    "    directory=test_dir,\n",
    "    x_col='img_name',\n",
    "    class_mode=None,\n",
    "    batch_size=BATCH_SIZE_INCEPTION,\n",
    "    shuffle=False,\n",
    "    target_size=(IMAGE_SIZE_INCEPTION, IMAGE_SIZE_INCEPTION)\n",
    ")"
   ],
   "metadata": {
    "id": "ensemble_datagen_code"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.3. Generate and Ensemble Predictions\n",
    "\n",
    "We use each model to predict the class probabilities for the entire test set. Then, we perform the key ensembling step: we calculate the element-wise average of the two prediction arrays. This resulting array represents the combined confidence of both models for each class on each image."
   ],
   "metadata": {
    "id": "part_4_3_markdown"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Generate predictions from DenseNet201\n",
    "print(\"Generating predictions with DenseNet201...\")\n",
    "pred_densenet = densenet_model.predict(test_generator_densenet, verbose=1)\n",
    "\n",
    "# Generate predictions from InceptionResNetV2\n",
    "print(\"\\nGenerating predictions with InceptionResNetV2...\")\n",
    "pred_inception = inception_model.predict(test_generator_inception, verbose=1)\n",
    "\n",
    "# Ensemble by averaging the predictions\n",
    "print(\"\\nAveraging predictions...\")\n",
    "final_predictions = np.mean([pred_densenet, pred_inception], axis=0)\n",
    "\n",
    "print(f\"\\nFinal predictions array shape: {final_predictions.shape}\")"
   ],
   "metadata": {
    "id": "ensemble_predict_code"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.4. Format and Save Final Submission File\n",
    "\n",
    "The final step is to format the predictions according to the competition's requirements. For each image, we need to find the top 3 most likely classes. \n",
    "\n",
    "1. We use `np.argsort` to get the indices of the classes, sorted from least to most likely.\n",
    "2. We take the last three indices (`[-3:]`) and reverse their order (`[::-1]`) to get the top 3.\n",
    "3. We map these indices back to their original class labels.\n",
    "4. Finally, we create a `submission.csv` file with the image name and the top 3 labels, separated by spaces."
   ],
   "metadata": {
    "id": "part_4_4_markdown"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Get the top 3 predicted class indices for each image\n",
    "top_3_indices = np.argsort(final_predictions, axis=1)[:, -3:][:, ::-1]\n",
    "\n",
    "# The generators create a class_indices_ dictionary. We need to invert it \n",
    "# to map from the predicted index back to the original class label.\n",
    "labels = (train_generator_densenet.class_indices)\n",
    "label_map = dict((v, k) for k, v in labels.items())\n",
    "\n",
    "# Map the indices to labels and format them as space-separated strings\n",
    "top_3_labels = []\n",
    "for i in range(len(top_3_indices)):\n",
    "    labels_for_image = [\n",
    "        label_map[top_3_indices[i][0]], \n",
    "        label_map[top_3_indices[i][1]], \n",
    "        label_map[top_3_indices[i][2]]\n",
    "    ]\n",
    "    top_3_labels.append(\" \".join(labels_for_image))\n",
    "\n",
    "# Create the submission DataFrame\n",
    "submission_df = pd.DataFrame()\n",
    "submission_df['img_name'] = test_generator_densenet.filenames\n",
    "submission_df['label'] = top_3_labels\n",
    "\n",
    "# Save the submission file in the required format\n",
    "submission_df.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "print(\"Submission file 'submission.csv' created successfully.\")\n",
    "submission_df.head()"
   ],
   "metadata": {
    "id": "ensemble_save_code"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Final Conclusion\n",
    "\n",
    "This project successfully implemented a robust, end-to-end pipeline for a complex fine-grained image classification task. By strategically applying **transfer learning** with two diverse architectures and enhancing model resilience through **data augmentation** and **model ensembling**, we created a high-quality solution well-suited to the challenges of the iFood 2019 dataset. The final `submission.csv` file is correctly formatted and demonstrates a powerful and well-reasoned approach to solving real-world computer vision problems."
   ],
   "metadata": {
    "id": "conclusion_markdown"
   }
  }
 ]
}
